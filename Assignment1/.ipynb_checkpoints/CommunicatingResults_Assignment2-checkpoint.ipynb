{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title of the Assignment\n",
    "\n",
    "* Student Name: NAME\n",
    "* Student Number: NUMBER\n",
    "* Date: DATE\n",
    "* Wordcount: (Include here the total words for the sections that have a wordcount limit)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Doing data analysis is not just about crunching the numbers, but also explaining what is being done, and why. Moreover, it is important to document your steps in a way that other analysts (or even yourself) can understand what was done, with what type of data, and based on what assumptions.\n",
    "\n",
    "This section briefly explains the communication challenge and proposes a general RQ so that the reader has enough context to understand the actual analysis being done in the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypotheses and Sub-Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the hypotheses (at least 2) that will be tested by the study. The section (a) defines the key concepts in each hypothesis, and (b) provides a theoretical justification/motivation for each hypothesis. Alternatively, Sub-RQs can also be proposed, but the text should be clear as to why it is needed to propose Sub-RQs instead of hypotheses and motivate each Sub-RQ adequately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "\n",
    "In the cells below you should load the dataset(s) into pandas to begin the data understanding and preparation. It is important to inform the reader basic information about each dataset, including:\n",
    "* What is the dataset about?\n",
    "* What is the source of the data (and/or how were the data collected)?\n",
    "    * If the data came from API's (e.g., Twitter, Facebook, YouTube) or specific tools (e.g., Google Analytics), this should be mentioned.\n",
    "    * If specific tools were used (e.g., DMI-TCAT for collecting Twitter data, or SentiStrength for sentiment analysis), they should also be mentioned (and the appropriate papers cited).\n",
    "* Why is the dataset being used?\n",
    "* When was the dataset last updated?\n",
    "\n",
    "For our assignment, if the dataset contains simulated data, we expect you to also include:\n",
    "* A clear disclaimer that this is a simulated dataset\n",
    "* A brief explanation of how the dataset was simulated\n",
    "\n",
    "\n",
    "As a tip: use a meaningful name for your dataset(s) when loading them into a Pandas dataframe. Calling them just *df* or *data* will become confusing later on.\n",
    "\n",
    "**IMPORTANT:** As part of responsible data analytics, it is important to indicate also to the reader where the data is stored or located. In the case of Assignment 2, we expect you to upload the dataset to a dropbox, so we can download the data and replicate your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "As discussed during the tutorials, the stages of data understanding (which we call *Data Exploration & Evaluation* in the evaluation criteria) and data preparation (which we call *Data Cleaning* in the evaluation criteria) are iterative steps, with a lot of back and forth until you have a usable dataset.\n",
    "\n",
    "In this stage, we expect you to meet the criteria included in the course guide. While doing this, you need to make sure that you are also clearly communicating what is being done in each step. This includes:\n",
    "* Identifying the key variables that your study will use\n",
    "* Explaining to the reader what these variables are\n",
    "* Performing the steps regarding data cleaning and data exploration as indicated on Canvas (Assignment 2 briefing)\n",
    "* Explaining to the reader at each step what is being done, and what the output/result means\n",
    "\n",
    "**IMPORTANT:**\n",
    "* You only need to do the cleaning and exploration for the variables that are *relevant* to your study. You can skip variables/columns in the dataset that are absolutely irrelevant to your study.\n",
    "* You need to communicate your steps clearly to the reader. This means that you should not just do a *df.describe()*  to communicate descriptive statistics in a dataset that contains a lot of irrelevant columns and expect the reader to figure out by her or himself what is relevant. You should instead only show to the reader what is relevant, and explain why.\n",
    "* If you use functions to categorize your data, you should explain how the functions were built (or where they came from), and what is being done.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration & Evaluation\n",
    "\n",
    "In this stage, *after the dataset is considered clean*, you are expected to show to the reader how the final dataset looks like, and perform an exploratory review of the data. At all times, you are expected to write a report-out to stakeholders (i.e., not only show the data, but also explain what the data show).\n",
    "\n",
    "This includes:\n",
    "* Reporting out of descriptives and definitions of all key variables in a clear and inteligible manner\n",
    "* Univariate visualizations for key variables \n",
    "* Visualization of key bivariate relationships (e.g., related to hypotheses or RQs)\n",
    "\n",
    "Key variables are all variables that are used as DV's or IV's in your hypotheses or RQs.\n",
    "\n",
    "\n",
    "**IMPORTANT:**\n",
    "* You only need to do the exploration for the variables that are *relevant* to your study. You can skip variables/columns in the dataset that are absolutely irrelevant to your study.\n",
    "* You need to communicate your steps clearly to the reader. This means that you should not just do a *df.describe()*  to communicate descriptive statistics in a dataset that contains a lot of irrelevant columns and expect the reader to figure out by her or himself what is relevant. You should instead only show to the reader what is relevant, and explain why.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Hypothesis Testing\n",
    "\n",
    "After understanding and preparing the data, you are ready to do the modeling. It is important to explain to the reader:\n",
    "* Explaining which models will be used, and why\n",
    "* If appropriate, explaining which strategy you are using to fine tune or improve the model\n",
    "* Discussion about the model evaluation (i.e., why you believe the model is a good model) and, if appropriate, comparison between different models\n",
    "* After the appropriate model is identified, then the hypothesis can be tested. You can then report the result of the tests being used to test the hypothesis (as discussed in the criteria for the assignment), explaining to the reader how the test is being made and what the results of the testing mean for the hypothesis at hand. \n",
    "* Visualizations (as appropriate) are helpful to explain to the reader what has been found.\n",
    "* Predicting probabilities (for interesting cases) might also be helpful (if appropriate)\n",
    "\n",
    "**IMPORTANT:**\n",
    "* We consider as different models both when different features/variables are being tested (to achieve the same objective), or when different algorithms are being tested (with the same features/IVs)\n",
    "* It is important to explain to the reader what the model contains/is, and also how it is being evaluated. When the evaluation is done, it is also important to discuss what the result means in the current context\n",
    "* While it is recommended to use models discussed in class for your analysis, you can also use ANOVAs or OLS regressions in SPSS if thatâ€™s more appropriate to the hypothesis testing at hand. The results, including screenshots, must be reported in the Jupyter Notebook. Logistic regression models are to be done in Pandas/Scikit-Learn/Statsmodels.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "This section provides the answer to the communication challenge, discussing the results (hypothesis testing) in order to answer the general RQ. Creativity in how to summarize the findings (including visualizations) is a plus.\n",
    "\n",
    "This section also provides a set of implications (i.e., now that we know the answer to the RQ, what does it mean for the organization/process/challenge? what should the organization do?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations and Next Steps\n",
    "\n",
    "In this section you should discuss all the relevant limitations to your analysis, including limitations or considerations about:\n",
    "* The dataset(s) itself, including how the data collection may have created limitations\n",
    "* Limitations associated with the decisions taken during the data understanding & preparation\n",
    "* Limitations about the model and hypothesis testing (including fit, selection of variables)\n",
    "* Alternative interpretations to the findings\n",
    "\n",
    "Next steps should also be suggested for further analysis (as indicated in the assignment criteria)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical, Privacy and Normative Considerations\n",
    "\n",
    "This section should discuss the considerations the organization needs to have. Are there ethical, privacy or more general normative\n",
    "aspects that need to be taken into account?\n",
    "This discussion can relate to the actual\n",
    "dataset/analysis that was done, to the\n",
    "recommendations/implications, and to the\n",
    "action plan suggested in the previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "If you use scientific papers, cite them using APA style (and add a reference section at the end). If you are using pieces of code written by someone else, add a comment in the appropriate section and add a link to the source.\n",
    "\n",
    "Regarding the tools we used:\n",
    "* Sentiment analysis was done using SentiStrength. The following paper can be cited:\n",
    "    * Thelwall, M., Buckley, K., Paltoglou, G., Cai, D., & Kappas, A. (2010). Sentiment strength detection in short informal text. *Journal of the American Society for Information Science and Technology*, 61(12), 2544-2558.\n",
    "\n",
    "* Tweets were collected using DMI-TCAT. The following paper can be cited:\n",
    "    * Borra, E., & Rieder, B. (2014). Programmed method: developing a toolset for capturing and analyzing tweets. *Aslib Journal of Information Management*, 66(3), 262-278.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
